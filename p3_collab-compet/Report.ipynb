{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sitting-holly",
   "metadata": {},
   "source": [
    "## Report - Collaboration and Competition - Multi-Agent task for the Tennis environment \n",
    "\n",
    "#### Author : Bhuvaneswari Sankaranarayanan, Prepared on April 27, 2021 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yellow",
   "metadata": {},
   "source": [
    "#### Learning Algorithm Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-radio",
   "metadata": {},
   "source": [
    "The Collaboration and Competition task for the Tennis environment is a two-agent system that aims to keep the ball in play for the maximum duration possible. The task has been solved using the Multi-Agent DDPG algorithm [MADDPG paper](https://arxiv.org/abs/1706.02275) using a fully connected deep neural network architecture to take in the 24-dimensional state and the 2-dimensional continuous action for each agent. \n",
    "\n",
    "MADDPG learns a critic network(local/target versions) and an actor network(local/target versions) for each agent but maintains a common replay memory that records the state-action-reward-next state tuple pairs of both the agents at any given time step. \n",
    "\n",
    "In this implementation, the critic network is a fully connected neural network taking the 24-dimensional state of both the agents (thus making it 48-dimensional flattened vector) and 2-dimensional action of both the agents (making it a 4-dimensional flattened vector) at the input. The input layer is followed by a batch normalization layer, a fully connected layer with 128 neuron units, a fully connected layer with 256 neuron units and another fully connected FC3 layer with 128 neuron units. The output of the critic network is a single real value that approximates the action-value function of the input state and action. The actor network is also a fully connected neural network that takes the state at the input and outputs the action to be taken at that state. It has a batch normalization layer, a fully connected layer of 128 neuron units and another fully connected hidden layer of 64 neuron units. The output of the actor network is the 2-dimensional action of the agent. \n",
    "\n",
    "In MADDPG, which is the multi-agent version of the DDPG algorithm, each agent learns and acts upon its own version of the critic and actor network. Simple backpropagation on appropriate loss functions as mentioned in the MADDPG paper was used to learn the actor and critic networks. Adam optimizer was used to perform the gradient updates at every update step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-scottish",
   "metadata": {},
   "source": [
    "<img src=\"./Figures_for_the_report/critic.png\" alt=\"Critic\" style=\"width: 400px;\"/> <img src=\"./Figures_for_the_report/actor.png\" alt=\"Actor\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-apartment",
   "metadata": {},
   "source": [
    "#### Hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-level",
   "metadata": {},
   "source": [
    "The diagrams above describe the NN architecture and hyperparameters like no. of hidden layers, no. of neuron units per layer etc. The following are the values of other hyperparameters used:-\n",
    "\n",
    "- Learning rate for the actor = 1e-4\n",
    "- Learning rate for the critic = 5e-3\n",
    "- Buffer Size = 1000000\n",
    "- Batch Size = 128\n",
    "- Tau (hyperparameter for soft update of the target network) = 0.001\n",
    "- Weight Decay (parameter of the PyTorch Adam optimizer) = 0\n",
    "- UPDATE_EVERY = 4 (update the actor/critic network for every 4 steps of the agent)\n",
    "- GAMMA, discount factor = 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-indonesian",
   "metadata": {},
   "source": [
    "#### Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-consumer",
   "metadata": {},
   "source": [
    "A plot of rewards per episode is included to illustrate that the agents are able to receive an average maximum reward(take the maximum reward of the two agents each episode and average this over the last 100 episodes) of at least +0.5 over the last 100 episodes . \n",
    "\n",
    "The screenshot of the jupyter notebook below has been obtained after training the MADDPG agent. It reports that the number of episodes needed to solve the environment ie. till the average reward crossed 0.5 was 5387 episodes. The average reward is calculated as the running average of rewards from the last 100 episodes.  \n",
    "![alt text](./Figures_for_the_report/screenshot_from_training_maddpg.png \"Screenshot of the average rewards over last 100 episodes displayed while training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-crawford",
   "metadata": {},
   "source": [
    "It also plots the rewards obtained in each episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-product",
   "metadata": {},
   "source": [
    "![alt text](./Figures_for_the_report/learning_curve_maddpg_general.png \"Learning Curve from training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-volleyball",
   "metadata": {},
   "source": [
    "#### Sample code to load the trained model for behaving in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "raised-national",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "# below code segment works for windows 10 system \n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "env = UnityEnvironment(file_name='./Tennis_Windows_x86_64/Tennis.exe')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "num_agents=2\n",
    "state_size=24\n",
    "action_size = 2\n",
    "\n",
    "from maddpg_agent import Agent\n",
    "agents = []\n",
    "for agent_id in range(num_agents):\n",
    "    agents.append(Agent(agent_id, num_agents, state_size, action_size))\n",
    "    \n",
    "# load the weights from file\n",
    "for agent_id in range(num_agents):\n",
    "    agents[agent_id].actor_local.load_state_dict(torch.load('./MADDPG_model_Solution1/checkpoint_actor_'+str(agent_id)+'.pth'))\n",
    "    agents[agent_id].critic_local.load_state_dict(torch.load('./MADDPG_model_Solution1/checkpoint_critic_'+str(agent_id)+'.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bsankaranarayanan2\\pycharmprojects\\udacity_drl_nd\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    while True:\n",
    "        actions=[]\n",
    "        for agent_id in range(num_agents):\n",
    "            actions.append(agents[agent_id].act(states[agent_id, :]))\n",
    "        actions = np.squeeze(np.array(actions), axis=1)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        if np.any(dones):\n",
    "            break \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-recipient",
   "metadata": {},
   "source": [
    "#### GIF of the trained agent's behaviour in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-broad",
   "metadata": {},
   "source": [
    "![alt text](video_of_learned_agents.gif \"GIF of the agent's play after learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-weather",
   "metadata": {},
   "source": [
    "#### Ideas for future work \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-dealing",
   "metadata": {},
   "source": [
    "Possible future work for this multi-agent task could be to try a few tweaks to the vanilla MADDPG algorithm.\n",
    "- Use prioritized experience replay to sample from the replay memory \n",
    "- Any improvements to the DDPG algorithm for a single agent environment would improve the MA-DDPG algorithm as well. For eg. the [Twin DDPG](https://spinningup.openai.com/en/latest/algorithms/td3.html) algorithm which uses Double Q-learning updates, employs a critic that updates twice as many times as the actor network, and adds noise to the action for a smoother Q function across actions(we already add noise in our implementation!).\n",
    "- Use task-specific information to modify the existing MADDPG algorithm. For eg. our tennis playing task is a fully co-operative learning task and hence it makes sense to use a common critic for both the agents to speed up learning.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
