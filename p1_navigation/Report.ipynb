{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sitting-holly",
   "metadata": {},
   "source": [
    "## Report - Navigation Project \n",
    "\n",
    "#### Author : Bhuvaneswari Sankaranarayanan, Prepared on March 14, 2021 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yellow",
   "metadata": {},
   "source": [
    "#### Learning Algorithm Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-radio",
   "metadata": {},
   "source": [
    "The Navigation project has been solved using a DQN learning agent which used a fully-connected deep neural network with a 37-dimensional input layer, 2 hidden layers, and a final 4-dimensional output layer for value function approximation. \n",
    "\n",
    "The 37-dimensional state vector obtained from the Banana Unity environment is fed as input to the network. The number of output units are 4, one unit to output the preference for each of the 4 actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-apartment",
   "metadata": {},
   "source": [
    "#### Hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-level",
   "metadata": {},
   "source": [
    "The following are the values of hyperparameters used:-\n",
    "- Learning Rate = 5e-4\n",
    "- Number of hidden layers = 2 \n",
    "- Number of neurons in each hidden layer = 64 \n",
    "- Replay Buffer Size = 100000\n",
    "- Batch Size = 64 \n",
    "- Tau (hyperparameter for soft update of the target Q function at every step) = 0.001\n",
    "- UPDATE_EVERY, update the Q network for every 4 steps of the agent\n",
    "- GAMMA, discount factor = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-indonesian",
   "metadata": {},
   "source": [
    "#### Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-crawford",
   "metadata": {},
   "source": [
    "A plot of rewards per episode is included to illustrate that the agent is able to receive an average reward (over 100 episodes) of at least +13. The screenshot of the jupyter notebook below has been obtained after training the DQN agent. It reports that the number of episodes needed to solve the environment ie. till the average reward crossed 13.0 was 545. It also plots the rewards obtained in each episode. Note that the average score or average reward is calculated as the running average of rewards from the last 100 episodes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-product",
   "metadata": {},
   "source": [
    "![alt text](Screenshot_of_output_from_training_1.png \"Learning Curve from training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-volleyball",
   "metadata": {},
   "source": [
    "#### Sample code to load the trained model for behaving in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "raised-national",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# below code segment works for windows 10 system \n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64/Banana.exe\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "from dqn_agent import Agent\n",
    "agent = Agent(state_size=37, action_size=4, seed=0)\n",
    "\n",
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "boxed-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-recipient",
   "metadata": {},
   "source": [
    "#### GIF of the trained agent's behaviour in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-broad",
   "metadata": {},
   "source": [
    "![alt text](video_clip_of_learned_agent.gif \"GIF of the agent's play after learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-weather",
   "metadata": {},
   "source": [
    "#### Ideas for future work \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-dealing",
   "metadata": {},
   "source": [
    "The benchmark average score of 13.0 can be achieved earlier by using additional improvements on top of DQN. \n",
    "\n",
    "1. Double Q Learning - Tabular methods have a guarantee of learning the target Q function exactly in the limit of infinite episodes. However, with function approximators there is always a noise between the estimated Q and the target Q. Although this noise could be zero-mean, Q learning ends up over-estimating the action values because it uses the max operation to estimate Q function at the next state's Q function and behaves greedily with respect to this estimated Q at the next action selection. That is, max operation causes overestimation because it does not preserve the zero-mean property of the errors of its operands. Modifying the existing DQN to implement Double DQN can estimate the action values better and lead to reaching the benchmark score earlier. \n",
    "\n",
    "2. Prioritized experience replay - This is essentially weighted sampling of the experiences in the replay memory proportional to the TD-error of the (S, A, R, S') sample. This can also improve the Q estimation on rare but important state-action pairs.  \n",
    "\n",
    "3. Duelling architecture for the Q network - Duelling variation of the Q-network can help because for many states, it is unnecessary to estimate the value of each action choice. \n",
    "\n",
    "A network that implements all of the above is also shown to be considerably out-performing on many Atari game examples. We can expect a similar effect for our task as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-spice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
