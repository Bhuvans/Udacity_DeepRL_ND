{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sitting-holly",
   "metadata": {},
   "source": [
    "## Report - Continuous Control task for the Reacher environment (Single Agent) \n",
    "\n",
    "#### Author : Bhuvaneswari Sankaranarayanan, Prepared on April 12, 2021 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-yellow",
   "metadata": {},
   "source": [
    "#### Learning Algorithm Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-radio",
   "metadata": {},
   "source": [
    "The Continuous Control task for the Reacher environment with a single agent has been solved using a DDPG learning agent which used a fully-connected deep neural network architecture to take in a 33-dimensional state and 4-dimensional action space. \n",
    "\n",
    "DDPG learns a critic network(local/target versions) and an actor network(local/target versions). In this implementation, the critic network is a fully connected neural network taking the 33-dimensional state and 4-dimensional action at the input, a fully connected FCS1 layer with 256 neuron units, a fully connected FC2 layer with 256 neuron units and a fully connected FC3 layer with 128 neuron units. The output of the critic network is a single real value that approximates the action-value function of the input state and action. The actor network is also a fully connected neural network that takes the state at the input and outputs the action to be taken at that state. It has a single hidden layer with 256 neuron units.   \n",
    "\n",
    "Simple backpropagation on appropriate loss functions as mentioned in the DDPG paper was used to learn the actor and critic networks. Adam optimizer was used to perform the gradient updates at every update step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-scottish",
   "metadata": {},
   "source": [
    "<img src=\"critic.png\" alt=\"Critic\" style=\"width: 400px;\"/> <img src=\"actor.png\" alt=\"Actor\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-apartment",
   "metadata": {},
   "source": [
    "#### Hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-level",
   "metadata": {},
   "source": [
    "The diagrams above describe the NN architecture and hyperparameters like no. of hidden layers, no. of neuron units per layer etc. The following are the values of other hyperparameters used:-\n",
    "\n",
    "- Learning rate for the actor = 1e-4\n",
    "- Learning rate for the critic = 1e-3\n",
    "- Buffer Size = 1000000\n",
    "- Batch Size = 64\n",
    "- Tau (hyperparameter for soft update of the target network) = 0.001\n",
    "- Weight Decay (parameter of the PyTorch Adam optimizer) = 0\n",
    "- UPDATE_EVERY = 4 (update the actor/critic network for every 4 steps of the agent)\n",
    "- GAMMA, discount factor = 0.99\n",
    "- Max_T, Maximum number of timesteps allowed per episode = 800\n",
    "- No. of episodes = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-indonesian",
   "metadata": {},
   "source": [
    "#### Plot of Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-consumer",
   "metadata": {},
   "source": [
    "A plot of rewards per episode is included to illustrate that the agent is able to receive an average reward (over 100 episodes) of at least +30. It is to be noted that this task is a continuing task and the end of the episode is marked by the max. number of timesteps defined as a hyperparameter in the code. \n",
    "\n",
    "The screenshot of the jupyter notebook below has been obtained after training the DDPG agent. It reports that the number of episodes needed to solve the environment ie. till the average reward crossed 30.0 was 2365. The average reward is calculated as the running average of rewards from the last 100 episodes.  \n",
    "![alt text](scrnshot_from_training.png \"Screenshot of the average rewards over last 100 episodes displayed while training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-crawford",
   "metadata": {},
   "source": [
    "It also plots the rewards obtained in each episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-product",
   "metadata": {},
   "source": [
    "![alt text](learning_curve.png \"Learning Curve from training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-cancer",
   "metadata": {},
   "source": [
    "The average reward dropped considerably around 2000 episode but later catched up to the 30.0 mark upon continuing learning. Either early stopping or decaying the learning rates over episodes could help the algorithm stabilize at the optimal solution. Running 3000 episodes on local computer's GPU took approximately 3 hrs to complete. Due to this large running time, the training was stopped once the average reward reached 30.0 and stayed close to it for few hundred episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-volleyball",
   "metadata": {},
   "source": [
    "#### Sample code to load the trained model for behaving in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "raised-national",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# below code segment works for windows 10 system \n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "env = UnityEnvironment(file_name='./Reacher_Windows_x86_64/Reacher.exe')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "from ddpg_agent import Agent\n",
    "agent = Agent(state_size=33, action_size=4, random_seed=0)\n",
    "\n",
    "# load the weights from file\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bsankaranarayanan2\\pycharmprojects\\udacity_drl_nd\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-recipient",
   "metadata": {},
   "source": [
    "#### GIF of the trained agent's behaviour in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-broad",
   "metadata": {},
   "source": [
    "![alt text](video_of_learned_agent.gif \"GIF of the agent's play after learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-weather",
   "metadata": {},
   "source": [
    "#### Ideas for future work \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-dealing",
   "metadata": {},
   "source": [
    "Possible future work for this continuous control task could be to try Proximal Policy Optimization (PPO). PPO methods have also been shown to do well for continuous control tasks. It would be a nice alternative to DDPG for the current reaching task. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
